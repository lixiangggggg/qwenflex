import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from collections import defaultdict
from functools import partial
import torch.nn.functional as F
import collections

def compute_importance(model_name="Qwen/Qwen2.5-0.5B", num_samples=50, device="cuda"):
    """
    è®¡ç®— Attention æ¨¡å— (o_proj) å’Œ MLP æ¨¡å— (down_proj) è¾“å‡ºçš„ç»å¯¹å¹³å‡æ¿€æ´»å€¼ä½œä¸ºé€šé“é‡è¦æ€§ã€‚
    """
    # æ£€æŸ¥ CUDA è®¾å¤‡
    if not torch.cuda.is_available() and device == "cuda":
        print("CUDA ä¸å¯ç”¨ï¼Œæ­£åœ¨ä½¿ç”¨ CPUã€‚")
        device = "cpu"

    print(f"âœ… æ­£åœ¨åŠ è½½æ¨¡å‹ï¼š{model_name} åˆ° {device}...")
    try:
        model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16).to(device)
        tokenizer = AutoTokenizer.from_pretrained(model_name)
    except Exception as e:
        print(f"âŒ æ¨¡å‹æˆ–åˆ†è¯å™¨åŠ è½½å¤±è´¥ï¼š{e}")
        return

    model.eval()

    # å­˜å‚¨æ¿€æ´»å€¼ï¼š{layer_name: {'attn': tensor, 'mlp': tensor}}
    activations = defaultdict(partial(defaultdict, float))

    # âœ… Attention hookï¼šæ³¨å†Œåœ¨ o_proj ä¹‹å
    def attn_hook(name):
        def hook(module, inp, out):
            with torch.no_grad():
                # out.shape: (batch_size, seq_len, hidden_size)
                # æ±‚æ‰€æœ‰æ ·æœ¬å’Œåºåˆ—ä½ç½®çš„å¹³å‡ç»å¯¹å€¼ï¼Œå¾—åˆ° hidden_size ç»´åº¦çš„é‡è¦æ€§
                val = out.detach().abs().mean(dim=(0, 1))
                activations[name]['attn'] += val.cpu()
        return hook

    # âœ… MLP hookï¼šæ³¨å†Œåœ¨ down_proj ä¹‹å
    def mlp_hook(name):
        def hook(module, inp, out):
            with torch.no_grad():
                # out.shape: (batch_size, seq_len, hidden_size)
                # æ±‚æ‰€æœ‰æ ·æœ¬å’Œåºåˆ—ä½ç½®çš„å¹³å‡ç»å¯¹å€¼ï¼Œå¾—åˆ° hidden_size ç»´åº¦çš„é‡è¦æ€§
                val = out.detach().abs().mean(dim=(0, 1))
                activations[name]['mlp'] += val.cpu()
        return hook

    # âœ… æ³¨å†Œ hook
    for i, layer in enumerate(model.model.layers):
        # Attention çš„è¾“å‡ºé€šé“ä¸ Attention å†…éƒ¨çš„ QKV é€šé“å¯¹åº” (hidden_size)
        layer.self_attn.o_proj.register_forward_hook(attn_hook(f"layer_{i}"))
        # MLP çš„è¾“å‡ºé€šé“ä¸ Attention çš„è¾“å‡ºé€šé“ç»´åº¦ç›¸åŒ (hidden_size)
        layer.mlp.down_proj.register_forward_hook(mlp_hook(f"layer_{i}"))

    texts = [f"ä»Šå¤©å¤©æ°”çœŸä¸é”™ï¼Œæˆ‘ä»¬ä¸€èµ·å»æ•£æ­¥å§ {i}" for i in range(num_samples)]
    print(f"âœ… æ­£åœ¨ç”¨ {num_samples} ä¸ªæ ·æœ¬è¿è¡Œå‰å‘ä¼ æ’­ä»¥æ”¶é›†æ¿€æ´»...")
    inputs = tokenizer(texts, return_tensors="pt", padding=True, truncation=True, max_length=64).to(device)

    with torch.no_grad():
        model(**inputs)

    torch.save(activations, "activations.pt")
    print("âœ… å·²ä¿å­˜æ¿€æ´»é‡è¦æ€§åˆ° activations.pt")


def reorder_qwen_weights(model_name="Qwen/Qwen2-1.5B", activation_path="activations.pt", save_path="./reordered2.5_qwen"):
    """
    æ ¹æ®æ¿€æ´»é‡è¦æ€§æ–‡ä»¶é‡æ’åº Qwen2 æ¨¡å‹çš„ Attention å’Œ MLP æƒé‡ã€‚
    """
    try:
        with torch.serialization.safe_globals([
            collections.defaultdict,
            partial
        ]):
            activations = torch.load(activation_path, weights_only=False)
    except Exception as e:
        print(f"âŒ æ¿€æ´»æ–‡ä»¶åŠ è½½å¤±è´¥ï¼Œè¯·å…ˆè¿è¡Œ compute_importanceï¼š{e}")
        return

    print(f"âœ… æ­£åœ¨åŠ è½½æ¨¡å‹ï¼š{model_name}...")
    try:
        model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16)
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥ï¼š{e}")
        return


    for i, layer in enumerate(model.model.layers):
        name = f"layer_{i}"
        if name not in activations:
            continue
        
        print(f"ğŸ”„ æ­£åœ¨å¤„ç† {name}...")

        # æ³¨æ„ï¼šæ­¤å¤„ä½¿ç”¨çš„ attn_imp å’Œ mlp_imp éƒ½æ˜¯ hidden_size ç»´åº¦çš„é‡è¦æ€§

        # å› ä¸º Qwen2 ä½¿ç”¨ GQA/MHAï¼Œå…¶å†…éƒ¨ key/value é€šé“æ•°é€šå¸¸ä¸ Attention çš„è¾“å‡ºé€šé“æ•°ä¸åŒã€‚
        # è¿™é‡Œä¸ºäº†ç®€åŒ–ï¼Œæˆ‘ä»¬ä»…é‡æ’ Q_proj å’Œ O_proj çš„é€šé“ã€‚
        # å¦‚æœéœ€è¦é‡æ’ K/Vï¼Œéœ€è¦æ ¹æ®æ¨¡å‹é…ç½®è®¡ç®—å®ƒä»¬å„è‡ªçš„é€šé“ç´¢å¼•ã€‚
        attn_imp = activations[name]['attn'] # shape: [hidden_size]

        # âœ… Attentionéƒ¨åˆ†æ’åº
        idx_attn = torch.argsort(attn_imp, descending=True)
        
        # Q_proj, K_proj, V_proj çš„å½¢çŠ¶ï¼š[num_heads * head_dim, hidden_size]
        Wq = layer.self_attn.q_proj.weight.data
        Wk = layer.self_attn.k_proj.weight.data
        Wv = layer.self_attn.v_proj.weight.data
        
        # O_proj çš„å½¢çŠ¶ï¼š[hidden_size, num_heads * head_dim]
        Wo = layer.self_attn.o_proj.weight.data
        
        # ä¿®æ­£ï¼šä½¿ç”¨ç´¢å¼•å¼ é‡å¯¹ç»´åº¦è¿›è¡Œç´¢å¼• (Attention çš„è¾“å‡ºé€šé“é‡æ’)
        # QKV çš„è¾“å‡ºç»´åº¦ï¼ˆè¡Œï¼‰å¯¹åº” Attention çš„è¾“å‡ºé€šé“ã€‚
        layer.self_attn.q_proj.weight.data = Wq[idx_attn, :]
        # layer.self_attn.k_proj.weight.data = Wk[idx_attn, :] # æš‚ä¸é‡æ’ K/Vï¼Œä¿ç•™åŸé€»è¾‘
        # layer.self_attn.v_proj.weight.data = Wv[idx_attn, :] # æš‚ä¸é‡æ’ K/Vï¼Œä¿ç•™åŸé€»è¾‘
        
        # ä¿®æ­£ï¼šO_proj çš„è¾“å…¥ç»´åº¦ï¼ˆåˆ—ï¼‰å¯¹åº” Attention çš„è¾“å‡ºé€šé“ã€‚
        layer.self_attn.o_proj.weight.data = Wo[:, idx_attn]


        # âœ… MLPéƒ¨åˆ†æ’åº
        mlp_imp = activations[name]['mlp'] # shape: [hidden_size]
        
        # up_proj çš„å½¢çŠ¶ï¼š[intermediate_size, hidden_size]
        W1 = layer.mlp.up_proj.weight.data
        
        # down_proj çš„å½¢çŠ¶ï¼š[hidden_size, intermediate_size]
        W2 = layer.mlp.down_proj.weight.data
        
        # æ ¹æ® MLP hook å¾—åˆ°çš„ hidden_size ç»´åº¦é‡è¦æ€§è¿›è¡Œæ’åº
        idx_mlp = torch.argsort(mlp_imp, descending=True)
        
        # é”™è¯¯è¡Œï¼šlayer.mlp.up_proj.weight.data = W1[:idx_mlp] 
        # ä¿®æ­£ï¼šup_proj çš„è¡Œï¼ˆè¾“å‡ºç»´åº¦ï¼‰å¯¹åº” intermediate_sizeï¼Œåˆ—ï¼ˆè¾“å…¥ç»´åº¦ï¼‰å¯¹åº” hidden_size
        # æˆ‘ä»¬å¯¹ W1 çš„ **åˆ—**ï¼ˆhidden_sizeï¼‰è¿›è¡Œé‡æ’
        layer.mlp.up_proj.weight.data = W1[:, idx_mlp]
        
        # é”™è¯¯è¡Œï¼šlayer.mlp.down_proj.weight.data = W2[idx_mlp]
        # ä¿®æ­£ï¼šdown_proj çš„è¡Œï¼ˆè¾“å‡ºç»´åº¦ï¼‰å¯¹åº” hidden_sizeï¼Œåˆ—ï¼ˆè¾“å…¥ç»´åº¦ï¼‰å¯¹åº” intermediate_size
        # æˆ‘ä»¬å¯¹ W2 çš„ **è¡Œ**ï¼ˆhidden_sizeï¼‰è¿›è¡Œé‡æ’
        layer.mlp.down_proj.weight.data = W2[idx_mlp, :]


    model.save_pretrained(save_path)
    print(f"âœ… å·²ä¿å­˜é‡æ’åºåçš„ Qwen æ¨¡å‹åˆ° {save_path}")


if __name__ == "__main__":
    # é…ç½®
    MODEL_NAME = "Qwen/Qwen2.5-0.5B"
    DEVICE = "cuda" # ç¡®ä¿æ‚¨çš„ç¯å¢ƒä¸­æ”¯æŒ CUDA
    NUM_SAMPLES = 50
    ACTIVATION_PATH = "activations.pt"
    SAVE_PATH = "./reordered_qwen"

    # 1. è®¡ç®—é‡è¦æ€§ (æ¿€æ´»å€¼)
    compute_importance(model_name=MODEL_NAME, num_samples=NUM_SAMPLES, device=DEVICE)
    
    # 2. é‡æ’åºæƒé‡å¹¶ä¿å­˜
    reorder_qwen_weights(model_name=MODEL_NAME, activation_path=ACTIVATION_PATH, save_path=SAVE_PATH)